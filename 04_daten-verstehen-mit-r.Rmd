---
title: "Daten verstehen mit R"
author: CorrelAid e.V.
date: "`r Sys.Date()`"
authors:
  - Nina Hauser
  - Zoé Wolter
  - Jonas Lorenz
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: flatly
    css: www/style.css
    includes:
      after_body: ./www/favicon.html
    language: de
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(learnr)
library(gradethis)
library(ggplot2)

source("R/setup/gradethis-setup.R")
source("R/setup/tutorial-setup.R")
# Read app parameters
params <- yaml::yaml.load_file("www/app_parameters.yml")

# Benötigte Daten laden
source("R/setup/functions.R")
community <- get_community()
```

```{r results='asis'}
cat(get_license_note(rmarkdown::metadata$title, rmarkdown::metadata$authors))
```

Wir wollen uns heute mithilfe von R einen Überblick über unsere Daten verschaffen. Wie uns das mit ein paar grundlegenden Schritten gelingt, werden wir anhand einiger Codes "in action" sehen. Wer möchte, kann diesen Code direkt replizieren, ansonsten soll es und erst einmal darum gehen, mit kleinen Handgriffen die Daten mithilfe von Funktionen und Kontextwissen greifbar zu machen. Beginnen wir doch einmal mit dem Video:

<!-- ![*Video: Daten mit R verstehen (30min)*](https://youtu.be/nEbiJ4EZ3OE) -->

# **Daten verstehen mit R**

## **Zusammenfassung: Grundlagen der Datenanalyse**

  - **Überblick**: Um explorative Datenanalysen durchführen zu können ist es wichtig, einen umfassenden Eindruck der Daten zu erhalten. Das gelingt uns, indem wir Muster erkennen und Extremwerte identifizieren können.
  - **Vorgehen**: Eine erste Datenanalyse lässt sich in drei grundlegende Schritte aufteilen. 
      1. Überblick mithilfe **niedrigschwelliger** Zusammenfassungen der Datene Daten zu verschaffen. 
      2. Untersuchung der Variablen mithilfe **einfacher Datenvisualisierungen**
      3. Weiterführende Analysen auf Basis der Berechnung **statistischer Kennzahlen**, (d.h. Lage- und Verteilungsparameter)

So weit so gut - allerdings gibt es noch eine Sache, die wir bei der Datenanalyse und -interpretation immer beachten müssen: **Den Kontext**! Ganz nach dem Grundsatz **Context is key!** achten wir also immer darauf, dass wir unsere Daten und die angewandten statistischen und visuellen Auswertungen immer im Zusammenhang zueinander bewerten. Dazu gehört ebenfalls, dass wir externe Rahmenbedingungen unserer Daten berücksichtigen: Woher kommen unsere Daten? Zu welchem Zweck wurden die Daten erhoben und aus welcher Zeit stammen die Daten? Wie könnten sich diese Aspekte auf unsere Daten auswirken?

## **Einstieg: Unsere Tools in R**

Keine Sorge, wir werden uns zu einem späteren Zeitpunkt noch einmal ganz ausführlich anschauen, wie wir spannende Visualisierungen erstellen, Daten(-formate) bearbeiten oder Reports erstellen und automatisieren können. Hier an dieser Stelle wollen wir uns erst einmal anschauen, wie wir in relativ kurzer Zeit mithilfe von R, Erkenntnisse aus unseren Daten ziehen können, um eine Fragen gezielt zu beantworten.

Dafür benötigen wir zunächst einmal zwei Packages: **`dplyr`** und **`ggplot2`**. Beide Packages sind Teil des sogenannten `tidyverse`-Packages, welches wir in den Lektionen zum Datenimport und zur Datentransformation noch genauer kennenlernen. An dieser Stelle nur einmal so viel: Das `tidyverse`-Package ist eine Sammlung verschiedener Packages, die uns das Leben in R in Zukunft sehr viel leichter machen werden und uns unglaublich viele Möglichkeiten bieten - also seid gespannt!
Für diese Woche werden wir zu allererst einmal wie gewohnt, die beiden Packages mit `install.packages("")`- und der `library()`-Funktion installieren und laden:

```{r pakete_ersteanalysen, exercise = TRUE}
# install.packages("dplyr")
# install.packages("ggplot2")

library(dplyr)
library(ggplot2)
```

Bevor wir nun direkt in unsere praktische Übung starten ein kleiner Test - aber nur ein kleiner! Für diese Session ist es nämlich praktisch, wenn wir uns wieder an unsere Lektion zu den **Grundlagen der Statistik** zurück erinnern. Viele Themen, Begriffe und Konzepte, die wir in unserer zweiten inhaltichen Sitzung gelernt haben, werden wir nämlich in dieser Woche einfach mit R umsetzen. Also: Was sind Daten, was sind Kennzahlen und welche Kennzahlen sind eigentlich für welche Daten wichtig?

## **Quiz**

```{r quiz_ersteanalyseninr}
quiz(caption = NULL,
  question("Was meint Ihr, welche Aussagen sind wahr?",
    answer("Die Berechnung statistischer Kennzahlen reicht aus, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Die Erstellung von Visualisierungen reicht aus, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Der Kontext ist bei der Analyse und Interpretation von Daten nicht so wichtig."),
    answer("Als Grundlage der Analyse und Interpreationen von Daten dienen uns die Berechnung statistischer Kennzahlen, sowie die Visualisierung und Kontextualisierung.", correct = TRUE),
    correct = "Richtig, es riecht eben nicht aus, Kennzahlen zu berechnen und bunte Grafiken zu erstellen. Statistische Maße und qualitative Visualisierungen sind wichtig und sind Teil einer umfassenden Datenanalyse. Während des gesamten Prozesses von der Datenanalyse bis hin zur Interpretation dürfen wir allerdings eines nicht aus den Augen verlieren: Den Kontext!",
    incorrect = "Leider falsch, die Berechnung statistischer Kennzahlen und die Visualisierung dieser sind ein wichtiger Teil einer umfassenden Datenanalyse. Während des gesamten Prozesses von der Datenanalyse bis hin zur Interpretation dürfen wir aber eines nicht vergessen: Den Kontext. Alle drei Dinge sind die Grundlage unserer Arbeit!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),

  question("Der Mittelwert ist eine statistische Kennzahl. Er ist robust gegen Ausreißer.",
    answer("Wahr."),
    answer("Unwahr.", correct = TRUE),
    correct = "Richtig, der Mittelwert ist eine Kennzahl, die wir umgangssprachlich gerne als Durchschnitt bezeichnen. Zur Berechnung summieren wir alle Werte auf und dividieren die Summe anschließend durch die Anzahl unserer Werte. Extremwerte - positiv wie negativ - haben dadurch einen großen Einfluss auf unseren Mittelwert. Wenn wir also das "wahre" Mittel unserer Werte bestimmen wollen, berechnen wir idealerweise den Median, dieser ist robust gegenüber Ausreißern!",
    incorrect = "Leider falsch, der Mittelwert ist eine Kennzahl, die wir umgangssprachlich gerne als Durchschnitt bezeichnen. Zur Berechnung summieren wir alle Werte auf und dividieren die Summe anschließend durch die Anzahl unserer Werte. Extremwerte - positiv wie negativ - haben dadurch einen großen Einfluss auf unseren Mittelwert. Wenn wir also das "wahre" Mittel unserer Werte bestimmen wollen, berechnen wir idealerweise den Median, dieser ist robust gegenüber Ausreißern!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),

  question("Wir haben die Verteilung einer Zufallsvariable: Der Mittelwert dieser Variable 5, die Spannweite 10 und die Standardabweichung 2. Um wie viel, weichen die Beobachtungen im Mittel von dem Wert 5 ab?",
    answer("2", correct = TRUE),
    answer("5"),
    answer("10"),
    correct = "Richtig, die Standardabweichung ist eine Maßzahl, die die mittlere Streuung bzw. Abweichung unserer Daten um den Mittelwert einer Variable angibt - in diesem Fall also um den Wert 2. Die Einheit der Standardabweichung entspricht dabei der Maßeinheit unserer Variable (Reminder: Die Varianz ist die quadrierte Standardabweichung und wird daher in einer anderen Einheit angegeben).",
    incorrect = "Leider falsch, die Standardabweichung ist eine Maßzahl, die die mittlere Streuung bzw. Abweichung unserer Daten um den Mittelwert einer Variable angibt - in diesem Fall also um den Wert 2. Die Einheit der Standardabweichung entspricht dabei der Maßeinheit unserer Variable (Reminder: Die Varianz ist die quadrierte Standardabweichung und wird daher in einer anderen Einheit angegeben).",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

# **Interaktive Übung**

## **1. Schritt der Datenanalyse: Fragen**

Am Anfang jeder Datenanalyse steht etwas, dass zunächst gar nichts mit Daten zu tun haben muss: **Fragen**. Wie wir bereits gelernt haben, sind Daten letztlich nichts anderes als **formalisierte Informationen**. Sie sollen uns also in erster Linie dabei helfen, durch die Analyse dieser Daten, einen **Erkenntnisgewinn** zu erlangen. Egal, ob wir zunächst einmal eigene Daten erheben wollen oder bereits daten zur Verfügung haben, strategische Überlegungen helfen uns dabei, bestimmte Fragestellungen zu formulieren auf die wir uns anschließend fokussieren können.
Bleiben wir also bei unserem [Datensatz](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-01-26){target="_blank"} von **[#Break Free From Plastic]**(https://www.breakfreefromplastic.org/){target="_blank"} und versetzen uns in die Rolle der Organisator:innen - welche Fragen könnten wir uns stellen und welche müssen wir uns sogar stellen? Wir haben hier ein paar Beispiele, ergänzt diese Liste gerne um Eure eigenen Ideen!

  1.  Wie viel Plastik wurde insgesamt gesammelt? <br>
  2.  Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>
  3.  Welche Faktoren beeinflussen möglicherweise diese Unterschiede? <br>

## **2. Schritt der Datenanalyse: Datenstruktur**

Nachdem wir diese Fragen formuliert haben, geht es darum, diese bestmöglich zu beantworten. Wir müssen also auf die Informationen schauen, die uns vorliegen, Das machen wir, indem wir unsere **Daten analysieren und interpretieren**! 

*Anmerkung: Unser Datensatz stammt aus den Jahren 2019 und 2020. Da es in dieser Einheit darum geht, in den **Analyse-Spirit** zu kommen und R als Analysetool kennenzulernen, haben wir den Datensatz "Break Free from Plastic" bereits **bereinigt** und in **zwei Datensätze** aufgeteilt: Einen `community`- und einen `audit`-Datensatz.
  - **`community´**: Dieser Datensatz enhtählt vor allem die Variablen, welche für Fragestellungen rund um die Community-Perspektive nützlich sind. Dazu zählen unter anderem die Anzahl an gesammelten Plastikteilen, die Anzahl der Events und freiwillig Engagierten.
  - **`audit`**: Dieser Datensatz umfasst vor allem jene Variablen, die für Fragen zur Audit-Perspektive nützlich sind. Dazu zählen unter anderem der Name der Firma und die Klassifizierung der Kunststoffe.*

### **Die `glimpse()`-Funktion**

Eine Funktion, die wir verwenden können, um uns einen ersten Überblick zu verschaffen, ist die **`glimpse()`-Funktion** aus dem Package `dplyr` (einem Package des `tidyverse` - mehr dazu in der kommenden Woche). Mit dieser Funktion erhalten wir Informationen über die Struktur des Datensatzes (Anzahl der Beobachtungen, Anzahl der Variablen, Name und Datentyp der Variablen). Der `community` Datensatz enthält geographische und zeitliche Variablen sowie Daten zur gesammelten Plastikmenge, Veranstaltungen und Freiwilligen. Wenn wir nun die Funktion **`dplyr::glimpse()`** ausführen und den Output betrachten, können wir verschiedene Dinge herauslesen, aber veruscht das einmal selbst:

```{r exercise_community, exercise = TRUE}
dplyr::glimpse(community)
```

```{r quiz_kurzstatistik}
quiz(caption = NULL,
  question(
    "Wie viele Variablen hat der `community`-Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("51"),
    answer("6", correct = TRUE),
    answer("7"),
    correct = "Richtig, unser Datensatz hat insgesamt sechs Variablen. Wir können das auf den ersten Blick daran erkennen, dass unser Datensatz aus eben seschs Spalten (Columns: 6) besteht - Wiederholung: Eine Spalte steht für ein Merkmal bzw. eine Variable. In den Zeilen werden im Gegensatz dazu die Beobachtungen abgebildet. In diesem Fall sind die Länder unsere Beobachtungseinheiten, also eine Zeile = ein Land.",
    incorrect = "Leider falsch, schau' nochmal einmal genauer hin und versuche dich daran zu erinnern, welche unterschiedlichen Informationen in den Zeilen und Spalten eines Datensatzes abgebildet werden - wir haben darüber in der Session "Grundlagen der Statistik" gesprochen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Welche Variablen gibt die Anzahl an gesammelten Plastikstücken an?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("n_volunteers"),
    answer("n_pieces", correct = TRUE),
    answer("n_events"),
    correct = "Richtig, die Variable "n_pieces" zeigt an, wie viele Plastikteile in dem jeweiligen Land gesammelt wurden. Wenn Du noch einmal eine Übersicht über die Bedeutung aller Variablennamen haben möchtest, kannst Du gerne jederzeit in das Codebuch schauen, das wir für Euch erstellt haben!",
    incorrect = "Leider falsch, schau' noch einmal genauer in die Übersicht, die dplyr::glimpse() ausgibt. Wenn Du noch einmal eine Übersicht über die Bedeutung aller Variablennamen haben möchtest, kannst Du gerne jederzeit in das Codebuch schauen, das wir für Euch erstellt haben!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
    )
  )
```

Fassen wir also noch einmal die Informationen zusammen, die wir auf den ersten Blick erkennen können:

  - Anzahl der Zeilen (Raws: 51) und Spalten (6): Eine Zeile ist eine Beobachtung, also in diesem Fall ein Land und eine Spalte steht für eine Variable
  - Bezeichnugen der Spalten, also die Namen der Variablen ($ continent, $ country, ...)
  - Ausschnitt der Ausprägungen der Variablen (z.B. für n_pieces: 988, 5818, 53)

## **3. Schritt: Inhaltlicher Überblick**

Wir haben nun also mithilfe der `glimpse()`-Funktion einen ersten Überblick über die Grundstruktur unseres Datensatzes und der darin enthaltenen Informationen erhalten, aber so richtig detailliert ist das Ganze jetzt noch nicht, oder? Zeit für eine neue Funtion!

### **Die `summary()`-Funktion**

Die **`summary()`-Funktion** ist eine grundlegende und vielseitige Funktion, die uns in kurzer Zeit eine **Zusammenfassung** über die Datenstruktur und -inhalte ermöglicht. Sie ist Teil des Base R-Packages - sie ist also bereits automatisch vorinstalliert und steht und zur Verfügung, ohne dass wir ein zusätzliches Package laden müssen. Sie fasst dabei grundlegende statistische Informationen zusammen und kann für Vektoren, Listen und Modelle verwendet werden - oder eben ganze Datensätze. Dabei gibt sie einen Überblick für jede einzelne Zeile unseres Datensatzes, wobei die tatsächlichen Informationen vom Datentyp (factor, numeric, etc.) abhängig sind. Schaut Euch doch den folgenden Output selbst einmal an: Welche Datentypen gibt es und welche Informationen stehen uns somit zur Verfügung? Welche Aussagen lassen sich treffen?

```{r summary_community, exercise = TRUE}
# Zusammenfassung
summary(community)
```

``{r quiz_plausibilisierung}
quiz(
  caption = NULL,
  question(
    "Was ist der niedrigste Wert, den die Variable n_pieces annimmt?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("68.5"),
    answer("1.0", correct = TRUE),
    answer("120646.0"),
    correct = "Richtig, werfen wir einen Blick auf die Variable 'n_pieces' und betrachten den ersten Wert, der als 'Min.' abgekürzt wird: Diese Abkürzung steht für 'Minimum' und gibt somit den kleinsten Wert unserer Variable an - in diesem Fall beträgt dieser 1!",
    incorrect = "Leider falsch, wir betrachten ausschließlich die Variable 'n_pieces'. Die Variablennamen werden zwar durch Abkürzungen dargstellt, aber welche könnte für den niedrigsten Wert stehen, der auch 'Minimum' genannt wird?",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Die Spannweite gibt die Differenz zwischen dem größten und dem kleinsten Wert einer Variable an. Wie plausibel findet Ihr die Spannweite der Variable n_pieces?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("Viel zu groß."),
    answer("Plausibel", correct = TRUE),
    answer("Viel zu klein"),
    correct = "Richtig, die Spannweite erscheint durchaus plausibel! Es gibt schließlich keine negativen Werte und auch die Größenordnung der Werte ist vorstellbar.",
    incorrect = "Leider falsch, betrachte noch einmal die Werte und achte darauf ob negative oder unrealistisch hohe Werte vorhanden sind. Wenn das in Ordnung aussieht, ist die Spannweite durchaus plausibel.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
    )
  )
```

Wir haben also auf einen Schlag schon eine ganze Menge Informationen erhalten. Beim Blick auf den Output fällt uns auch direkt auf, dass die verschiedenen Variablen unterschiedliche Formate (auch: Datentypen) haben. Die Merkmale `continent`, `country` und `countrycode` sind Variablen der Klasse "character". Sie bestehen aus Text/Buchstaben und sind statistisch gesehen **nominal skalierte** Variablen. Das bedeutet, dass wir sie zwar in Kategorien einsortieren, ihnen aber keine numerischen Werte zuweisen oder die in eine Rangliste bringen können. Dies können wir allerdings mit allen anderen Variablen tun, denn die `n_...` sind **metrisch skalierte** Variablen, die wir eben auch der Größe nach sortieren und für unsere weiteren Berechnungen verwenden dürfen.

### **Die `sum()`-Funktion**

Mithilfer der `summary()`-Funktion haben wir schon viele spannende Informationen über die numerischen Werte (`n_...`) erhalten. Die **Gesamtsumme** der gesammelten Plastikteile, durchgeführten Events oder freiwillig Engagierten. Müssen wir dafür nun Zeile für Zeile durchgehen und die Gesamtzahl mühsam mit dem Taschenrechner berechnen? Natürlich nicht, auch hier können wir eine einzige Funktion verwenden, die auch bereits in R vorinstalliert ist: Die **`sum()`-Funktion**. Innerhalb der Funktion verwenden wir an dieser Stelle auch das `$`-Zeichen, das wir bereits einmal kurz ausprobiert hatten. Damit können wir auf bestimmte **Variablen verweisen**. Dazu geben wir lediglich den Datensatz, in dem unsere Variable enthalten ist, sowie den Namen der entsprechenden Variable an:

```{r berechnung_summen, exercise = TRUE}
# Gesamtsumme
sum(community$n_pieces)
sum(community$n_events)
sum(community$n_volunteers)
```

### **Die `summarize()`-Funktion** 

Wir haben nun mithilfe der `glimpse()`-, `summary()`- und `sum()`-Funktion eine erste Überischt über unsere Struktur und Inhalte des Datensatzes erhalten. Als nächstes wollen wir wir Daten in Tabellen zusammenfassun, um beispielsweise neue Datenobjekte zu erstellen. Dabei greifen wir auf eine weitere Funktion aus dem `dplyr`-Package zurück: Die **`summarize()-Funktion** (auch: `summarise()`). Mit dieser Funktion können wir auf Basis der bereits vorhandenen Variablen neue **Hilfsvariablen** erstellen, die Berechnungen für uns zusammenfassen. Im folgenden Beispiel erstellen wir eine Übersicht, welche die Gesamtzahl, sowie die Spannweite der gesammelten Plastikteile berechnet. Hierzu bennen wir zwei neue **Hilfsvariablen 'Gesamt' und 'Spannweite'** und weisen diesen dann die gewünschte Funktion oder Funktionskombination zu:

```{r summarise, exercise = TRUE}
dplyr::summarize(community, 
                 Gesamt = sum(community$n_pieces),
                 Spannweite = max(community$n_pieces) - min(community$n_pieces))
```
*Übersetzung: Nutze den Datensatz 'community' und erstelle eine Variable 'Gesamt', die alle gesammelten Plastikteile aufsummiert und eine Variable 'Spannweite', die sich aus der Differenz aus Maximum und Minimum der Variable 'n_pieces' berechnet.* 

### **`summarize()` und `group_by()`**

Was hier nun passiert ist: alle Länder wurden natürlich zusammengeschmissen... - und obwohl das natürlich auch einen Einblick in unsere Daten verschafft, können wir hiermit noch lange keine Muster erkennen. Dafür müssen wir die `summarize()`-Funktion um eine weitere ergänzen: Die **`group_by()`-Funktion** aus dem `dplyr`-Package. Beide Funktionen werden gerne miteinander kombiniert, da sie uns gemeinsam in recht kurzer Zeit etwas umfassendere Zusammenfassungen über die Daten erstellen. Die `group-by`-Funktion erlaubt es uns, Daten nach en Werten einer oder mehrerer Spalten zu kategorisieren und somit Gruppenanalysen zu erstellen. 
Wenn wir also sehen möchten, wie viele Länder pro Kontinent an den Aktionen teilgenommen haben, wie viel insgesamt gesammelt wurde, und wie groß der Unterschied zwischen den Ländern ist, müssen wir lediglich den oberen Code ein wenig ergänzen. Zunächst einmal müssen wir mithilfe von `group_y()` festlegen, auf welchen Datensatz wir zugreifen (`community`) und dass wir diese Daten anhand der neuen Hilfsvariable 'Kontinent' kategorisieren wollen. Danach folgt noch die neue Hilfsvariable 'Länder', da wir uns für die länderübergreifenden Unterschiede interessieren, sowie die beiden Hilfsvariablen, die wir bereits kennen.

```{r Vorschau_group_by, exercise = TRUE}
# Kombination group_by und summarize
dplyr::summarize(group_by(community, Kontinent = community$continent), # aus den Daten 'community' eine Kategorie nach 'continent'
                 Länder = n(), # Anzahl der Zeilen
                 Gesamt = sum(n_pieces), # Summe der Werte
                 Spannweite = max(n_pieces) - min(n_pieces)) # Differenz max() und min()
```

Somit können wir auf jeden Fall einige weitere Informationen aus unseren Daten gewinnen: In Asien wurde beispielsweise am meisten Plastik gesammelt, jedoch finden wir hier auch die größten Unterschiede zwischen den Ländern (d.h., hohe Streuung der Beobachtungen). In Europa wurde zwar insgesamt nicht so viel gesammelt, wobei die teilnehmenden Länder hier alle ähnlich viel gesammelt haben. Mithilfe dieser Zahlen können wir also bereits einige unserer Fragen beantworten und eigene Erkenntnisse gewinnen. Wir sollten allerdings auch die Hinweise erkennen, die uns die Daten darüberhinaus liefern, nämlich dass es nicht ganz so einfach ist, sie miteinander zu vergleichen - zumindest nicht auf dieser Ebene.

### **Vorsicht: Interpretation der Daten!**

Wie gesagt, wir können bereits jetzt schon wertvolle Erkenntnisse aus unseren Daten gewinnen, aber: Besonders bei der der Interpretation unserer Daten müssen wir auch stets die **Aggregationsebene** dieser im Auge behalten. Man verweist in diesem Zuge auch auf die **Granularität der Daten**: Je aggregierter Daten vorliegen, desto schwieriger wird es, Zusammenhänge korrekt zu bestimmen. Es kommt also auch hier wieder auf den **Kontext** drauf an!
Wir müssen uns also Gedanken darüber machen, wie wir den Kontext unserer Daten an dieser Stelle beschreieben können. In unserem Datensatz können wir beispielsweise lediglich Vergleiche zwischen verschiedenen Ländern anstellen - aber können wir auch Aussagen über individuelle Events treffen? Nein, denn auf dieser Ebene liegen uns schlicht keine Informationen vor! Diese Schwankungen und ihre möglichen Ursachen müssen wir also stets bedenken. Die Anzahl der durhcgeführten Events weicht beispielsweise stark von der Anzahl involvierter Helfer:innen ab. Wie genau diese Unterschiede zu werten sind, ob beide Zahlen von anderen Variablen (sog. Störfaktoren, engl. *confounding variables*) beeinflusst werden und ob wir diese überhaupt auf Basis unserer Daten bewerten können müssen wir noch herausfinden. 
Ein Schlagwort welches häuftig in Verbindung mit der Aggregation von Daten fällt, ist der **"Ökologische Fehlschluss"**. Dieser beschreibt, wie auf der Basis von Aggregatdaten, die Merkmale eines Kollektivs abbilden, unzulässigerweise auf die Individualebene geschlossen wird. Mehr dazu findet Ihr [hier](https://martin-grellmann.de/oekologischer-fehlschluss).






#### Datenvisualisierung

Neben Tabellen sind auch einfache Datenvisualisierungen für das Datenverständnis hilfreich! So lassen sich Daten visuell darstellen und statistische Muster erkennbar machen. Für diesen explorativen Schritt der Datenanalyse müsst Ihr nicht auf die Gestaltung oder den genauen Code der Plots achten - es soll vorrangig um das Erkennen vom Datenverhalten gehen.

Erstellen wir zur Betrachtung der Kontinente als erstes ein Punktediagramm (engl. *Scatterplot*). Hierzu nutzen wir das Package `ggplot2`, mit dem man einen Plot über eine Art Schichtsystem aufbaut. Die erste Schicht, enthalten in der `ggplot()` Funktion, verweist auf den Datensatz (*community*) und die bestimmten Variablen (*continent*, *n_pieces*), die entlang der x- und y- Achsen dargestellt werden sollen. Die `geom_jitter()` Funktion bestimmt die Art der Visualisierung: *jitter* beschreibt eine besondere Art von Punktediagramm, bei dem Datenpunkte sich ausweichen um die Lesbarkeit bei Überlagerung zu ermöglichen. Andere Schichten, hinzugefügt über "+", beinhalten Schichten zur Gestaltung: Annotation, Layout, etc.

Diese Visualisierung stellt sowohl die Verteilung als auch die Häufigkeit von Beobachtungen über die Verteilung dar. Nehmt Euch einen Moment und beschreibt den Plot in Euren Worten.

Wo würdet Ihr die jeweiligen Mittelwerte verorten? <br>

```{r geom_point_n_pieces_bericht, exercise = TRUE}
# Erstellung eines jitter plots zur Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, 
       aes(x = continent, # x-Achse soll Kontinente zeigen
           y = n_pieces)) +  # y-Achse soll Stücke zeigen
  geom_jitter(size = 3, # Größe der Punkte
             alpha = 0.6, # Transparenz der Punkte
             width = 0.2) +  # Breite der Punkt-jitter pro Kategorie
  labs(title = "Auch die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent und Land.",
    y = "Anzahl gefundener Plastikstücke pro Land",
    x = "Kontinent",
    caption = "Datenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none") # Ausblenden der Legende
 
```

```{r quiz_scatterplot}
quiz(caption = NULL,
question(
    "In welchen Kontinenten beobachten wir extreme Werte, sogenannte 'Ausreißer'?",
    answer("Afrika und Amerika"),
    answer("Afrika und Europa"),
    answer("Afrika und Asien", correct = TRUE),
    answer("Afrika und Ozeanien"),
    correct = "Richtig!",
    incorrect = "Leider falsch: in Afrika und Asien sehen wir einige sehr große Werte, die weit von den anderen Datenpunkten entfernt sind.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```


#### Betrachtung von statistischen Kennzahlen

Um gezielte Aussagen zum Datenverhalten machen zu können, helfen uns statistische Maße. Idealerweise interpretiert Ihr statistische Kennzahlen im Zusammenspiel mit weiteren Werten zum Datensatz: wie viele Zeilen (Beobachtungen) tragen zur Statistik bei und wie verteilen sich diese innerhalb der Variablen? Kurz: Wie ist der Datenkontext?

Mit der oben erwähnten Kombination aus `dplyr::group_by()` und `dplyr::summarize()` können wir weitere statistische Kennzahlen auf einen Schlag ermitteln und so folgende Fragen beantworten: Wie stark streuen die beobachteten Werte um den Mittelwert, wie hoch sind Standardabweichung und Varianz? Gibt es Ausreißer?

```{r ueberblick_statistische_kennzahlen, exercise = TRUE}
# Tabelle mit statistischen Kennzahlen
summarize(group_by(community, Kontinent = community$continent), 
         "Anzahl Länder" = n(),
         "Anzahl Plastikstücke" = sum(n_pieces),
         "Mittelwert" = mean(n_pieces),
         "Standardabweichung" = sd(n_pieces),
         "Varianz" = var(n_pieces),
         "Median" = median(n_pieces),
         "Quartil (25%)" = quantile(n_pieces, .25),
         "Quartil (75%)" = quantile(n_pieces, .75),
         "Interquartilsabstand (IQR)" = IQR(n_pieces),
         "Spannweite" = max(n_pieces) - min(n_pieces)) 
```

**Aber was bedeuten diese Kennzahlen nochmal genau?**

*Wenn ihr Euch nicht mehr sicher seid, schaut doch nochmal in die Lektion zu den Grundlagen der Statistik. Weiter unten findet Ihr als kleine Hilfestellung auch eine zusammenfassende Abbildung, die die statistischen Werte nochmal gegenüberstellt.*

```{r quiz_standardabweichung}
quiz(caption = NULL,
  question("Warum gibt R für die Standardabweichung für Ozeanien (eng. Oceania) 'NA' aus?",
    answer("Weil keine Plastikstücke gefunden wurden."),
    answer("Weil Ozeanien nur drei Stück Plastik registriert hat."),
    answer("Weil nur ein Land in Ozeanien mitgemacht hat.", correct = TRUE),
    answer("Kann eigentlich nicht sein. Deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Ozeanien hat nur ein Land mitgemacht, weshalb es zwischen einzelnen Beobachtungen auf Länderebene natürlich keine Standardabweichung geben kann.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```


Eine Visualisierung, die viele nützliche statistischen Kennzahlen aufzeigt, ist der sogenannte **Boxplot**. Ein Boxplot stellt die Verteilung von Variablen inklusive wichtiger statistischer Eigenschaften dar und fasst die **fünf Punkte der Verteilung (Minimum, 25%-Quartil, Median, 75%-Quartil, Maximum)** zusammen. Aber Vorsicht: "einfache" Boxplots zeigen ausschließlich die statistischen Werte, nicht aber die einzelnen Datenpunkte. Diese sind aber wichtig um zu erkennen, wie die tatsächliche Verteilung ausschaut (bspw. ob sich die "Box" aus 4 oder 400 Werten zusammensetzt), Stichwort: Kontext! In R lassen sich Boxplots und Scatterplots zum Glück gut kombinieren.

Schauen wir uns das mal am Beispiel für Amerika an. Jeder Punkt bezieht sich hierbei auf die Anzahl der Plastikstücke für ein bestimmtes Land aus Amerika. Die statistischen Werte können aus diesem, auf der Seite liegenden, Boxplot ausgelesen werden:

![Boxplot mit statistischen Kennzahlen](https://github.com/CorrelAid/lernplattform/blob/main/abbildungen/04_daten-verstehen/Boxplot_Stat.png?raw=true){#id .class width="100%" height="100%"}

Der **Median** (hier in grün) stellt die **wahre Mitte der Verteilung** dar und ist die Beobachtung, die die Reihe aller enthaltenen Werte, also aller Länder, genau in zwei Hälften teilt: In Amerika haben 7 Länder an **Break Free From Plastic** teilgenommen. Die Anzahl an Plastikstücken des vierten Landes in der imaginären Aufreihung ist der Median.

Die **Spannweite** ist die Differenz aus Minimum und Maximum: 100% oder schlicht "alle" beobachten Werte liegen darin. Allein hat sie allerdings nur eine geringe Aussagekraft. Eine hohe Spannweite kann auf eine erklärbare Varianz hinweisen (beispielsweise, dass in Amerika manche Länder extrem viel gesammelt haben, und andere dafür gar nicht) - aber eine hohe Spannweite kann auch ein Zeichen dafür sein, dass die Daten von anderen Faktoren abhängig sind. Eine geringe Spannweite zeugt davon, dass die Datenpunkte in sich eher homogen sind.

Wie schauen die anderen Kontinente in der **Boxplotdarstellung** aus? Versucht mal die statistischen Kennzahlen für die Kontinente zu beschreiben - und was sich daraus für Schlussfolgerungen ergeben!

*(Hinweis: wie Ihr auch so einen Boxplot erstellen könnt, lernt Ihr in der Lektion zu Datenvisualisierung! Hier geht's ja erstmal darum, zu sehen, wie uns R helfen kann Daten zu lesen, zu kontextualisieren und zu interpretieren.)*

```{r boxplot_plastik}
# Erstellung eines Boxplots mit Scatterplot zur Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, 
       aes(x = continent, # x-Achse
           y = n_pieces,  # y-Achse
           fill = continent)) + # Farb-füll-Variable
  geom_boxplot(alpha = 0.6) + # Hinzufügen des Boxplots
  geom_point(size = 3, # Größe der Punkte
              alpha = 0.4, # Transparenz der Punkte
              width = 0.1) +  # Breite der Punkt-jitter pro Kategorie
  coord_cartesian(ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + # Festlegung der Achsenlänge der y-Achse abhängig von Median und Standardabweichung
  labs(
    title = "Die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl gefundener Plastikstücke",
    x = "Kontinent",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none") + # Ausblenden der Legende
  scale_fill_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67")) # Anwendung der BFFP-Farben
```

```{r quantile}
quiz(caption = NULL,
 question("Was trifft nicht auf das 50%-Quantil zu?",
    answer("Auch als Median bekannt."),
    answer("Nicht so sensibel Ausreißern gegenüber wie das arithmetische Mittel."),
    answer("Entspricht stets dem arithmetischem Mittel.", correct = TRUE),
    answer("50% aller Werte sind kleiner (gleich) diesem Wert."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Median und Mittelwert können bei symmetrischen Verteilungen identisch sein, in der Regel unterscheiden sie sich aber.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
 
 question("Was trifft auf Boxplots zu?",
    answer("Wir können daraus den Mittelwert ablesen."),
    answer("Alle Beobachtungen liegen innerhalb der 'Box'."),
    answer("Sie fassen 5 Punkte einer Verteilung zusammen.", correct = TRUE),
    answer("Boxplots kann man auch auf nominalskalierte Variablen wie Kontinente anwenden."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Boxplot ist nur auf mindestens ordinalskalierte Variablen anwendbar und geben den Median aus. Ausreißer können auch außerhalb der Whisker abgetragen sein.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
 
  question("Was sind die fünf Punkte einer Verteilung, welche im Boxplot immer dargestellt werden?",
    answer("Minimum", correct = TRUE),
    answer("25%-Quantil", correct = TRUE),
    answer("Mittelwert"),
    answer("Median", correct = TRUE),
    answer("75%-Quantil", correct = TRUE),
    answer("Maximum", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: nur wenn Mittelwert und Median identisch sind, wird auch der Mittelwert erfasst.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)

```

---

<details>
  <summary><h4>&#10145; Exkurs: Beziehungen zwischen Variablen</h4></summary>
  <br>
  
<h4><b>Exkurs: Beziehungen zwischen Variablen</b></h4> 

Bisher haben wir uns vor allem der **univariaten** (= eine Variable) Verteilung von Plastikstücken gewidmet. Nun möchten wir die Daten weiter nutzen, um **bivariat** (= zwei Variablen) herauszuarbeiten: Welche Faktoren beeinflussen möglicherweise die Unterschiede in der Anzahl an Plastikstücken, die gesammelt wurden? Vielleicht die Zahl an Events? Oder ist es vielleicht eher die Zahl an Freiwilligen im Land?

Für Fragestellungen zu möglichen Korrelationen zwischen zwei Variablen kann uns ein Scatterplot Aufschluss geben. Hier testen wir, ob die Anzahl an Events und/oder die Anzahl an Freiwilligen möglicherweise die Anzahl der gefundenen Plastikstücke beeinflusst. Eine Hilfslinie zeigt die Korrelation dieser beiden Variablen. Vergleichen wir mal...

<div style='float: left; width: 50%;'>
```{r scatter_plot_n_events, fig.width=4,fig.height=3}
# Optional: Erstellung eines Punktediagramms mit der Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, aes( x = n_events, 
                              y = n_pieces)) + 
  geom_point(size = 3,
             alpha = 0.5, 
             color = "darkgrey") + 
  geom_smooth(method = "lm", 
              colour = "darkred", 
              alpha = 0.5, 
              size = 1.5,
              se = F) + # Trendlinie hinzufügen, ohne Standardabweichung (se)
  coord_cartesian(xlim = c(0, median(community$n_events) + 2 * IQR(community$n_events)), 
                  ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + 
  # Festlegung der Achsenlänge der y-Achse abhängig von Median und Interquartilabstand
  labs(
    title = "Anzahl gesammelter Plastiksstücke bei \n'Break Free From Plastic' ..." ,
    subtitle = "... in Abhängigkeit von der Eventanzahl.",
    x = "Events",
    y = "Anzahl gefundener Plastikstücke",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() # Festlegung des Layout-Designs
```

</div>

<div style='float: right; width: 50%;'>
```{r scatter_plot_n_volunteers, fig.width=4,fig.height=3}
# Optional: Erstellung eines Punktediagramms mit der Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, aes( x = n_volunteers, 
                              y = n_pieces)) + 
  geom_point(size = 3,
             alpha = 0.5, 
             color = "darkgrey") + 
  geom_smooth(method = "lm", 
              colour = "darkred", 
              alpha = 0.5, 
              size = 1.5,
              se = F) + # Trendlinie hinzufügen, ohne Standardabweichung (se)
  coord_cartesian(xlim = c(0, median(community$n_volunteers) + 2 * IQR(community$n_volunteers)), 
                  ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + 
  # Festlegung der Achsenlänge der y-Achse abhängig von Median und Interquartilabstand
  labs(
    title = "Anzahl gesammelter Plastiksstücke bei \n'Break Free From Plastic' ..." ,
    subtitle = "... in Abhängigkeit von der Anzahl der Freiwilligen",
    x = "Freiwillige Helfer*innen",
    y = "Anzahl gefundener Plastikstücke",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() # Festlegung des Layout-Designs
```
</div>

Wenn wir uns das Punktediagramm ansehen, dass die Anzahl an Plastikstücken der Anzahl an Freiwilligen gegenüberstellt, ist ein etwas deutlicherer Zusammenhang als bei der Anzahl an Events erkenntlich: die Trendlinie zeigt an, dass mit Zunahme von einem Faktor, auch der andere ansteigt. Aber Achtung, denn unser Diagramm sagt nicht mit Sicherheit aus, dass mehr Freiwillige die *Ursache* für mehr gesammelte Plastikstücken sind. Wir beobachten sie lediglich gleichzeitig. Außerdem muss auch hier wieder beachtet werden, dass die Aussage nur auf Länderebene getroffen werden kann: es kann nicht geschlossen werden, dass Events mit mehr Freiwilligen mehr Plastikstücke sammeln. Wir erinnern uns an das Stichwort "Ökologischer Fehlschluss" von oben.

Was nehmen wir aus dieser bivariaten Analyse mit? Die Anzahl an Events korreliert nur sehr schwach positiv mit der Anzahl gesammelter Plastikstücke. Die Anzahl der Freiwilligen korreliert stark positiv mit der Anzahl an gesammelten Plastikstücken. Für diesen Zusammenhang kann es viele Ursachen geben, die wir in unseren Daten überhaupt nicht betrachten, zum Beispiel die Bevölkerungsanzahl oder auch die Zeit, welche einzelene Freiwillige investiert haben.

Welche "Störfaktoren" fallen Euch noch ein?

</details>

---


### **Und jetzt Ihr**
Diese Woche möchten wir die Präsenzzeit nutzen, um die folgenden Übungen zu besprechen. Ergänzt unseren Input gerne zudem mit Euren **Ideen, Fragen, Anregungen oder Kommentaren**. Es ist nicht schlimm, falls diese Woche noch gar nichts (komplexes) klappt, da wir das Gelernte in den nächsten Wochen wiederholen und vertiefen werden.

1.  Beantwortet anhand der präsentierten Datenanalyse die vorgestellten **Fragen**: <br>

-   Wie viel Plastik wurde insgesamt gesammelt? <br>
-   Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>
-   Welche Faktoren beeinflussen möglicherweise diese Unterschiede? <br>

2.  Überlegt: Mit welchen Daten und Datenanalysen könnte die Frage "Wie erfolgreich war der Audit?" noch beantwortet werden? Wie könnte eine Visualisierung oder eine zusammenfassende Statistik dabei helfen? Skizziert Eure Fragen gerne schriftlich.

3.  Versucht, das zugehörige **R Markdown: 06_daten-verstehen-mit-r-uebung.Rmd** (im [Übungsordner](https://download-directory.github.io/?url=https://github.com/CorrelAid/lernplattform/tree/main/uebungen){target="_blank"} unter 06_daten-verstehen-mir-r) zum Laufen zu bringen und es nachzuvollziehen.

4.  In der ersten Einheit haben wir uns bei der Visualisierung vor allem der **n_pieces Variable** gewidmet. Nun blicken wir auf die **n_volunteers**: Wie sehr unterscheiden sich die Freiwilligenzahlen nach Kontinenten? Erstellt in dem heruntergeladenen RMarkdown ein **Punktediagramm** (Scatterplot) mit dem Datensatz `community` für diesen Blickwinkel auf den Erfolg der "Break Free from Plastic" Aktion. Die Graphik soll `n_volunteers`, die **Anzahl der Freiwilligen** auf der y-Achse und die **Kontinente** auf der x-Achse zeigen. *Hinweis: Versucht dazu im RMarkdown in der finalen Version der Graphik die entsprechenden Variablen auszutauschen (und sonst erstmal nichts).*

5.  Interpretiert die Graphik. Was könnt Ihr ablesen?

### **Zusätzliche Ressourcen**

-   Die kostenlosen Kurse des [Statistischen Bundesamts](https://www.destatis.de/DE/Service/Statistik-Campus/E-Learning/eLearning-statistik.html;jsessionid=63AE25DDABD8853990FBE83F354C8911.live722?nn=206328){target="_blank"}
-   Stocker T. C. und Steinke I. (2017): Statistik – Grundlagen und Methodik [verfügbar z.B. hier](https://www.beck-shop.de/stocker-steinke-de-gruyter-studium-statistik/product/32926361){target="_blank"}
-   [R for Data Science (engl.)](https://r4ds.had.co.nz/){target="_blank"}
-   [Statistics Fundamentals in R](https://app.dataquest.io/course/statistics-fundamentals-r){target="_blank"} auf DataQuest (engl.)
-   [Lernvideos](https://www.youtube.com/watch?v=RRIsBFW8ovc){target="_blank"} zur Inferenzstatistik (dt.)

<a class="btn btn-primary btn-back-to-main" href=`r params$links$end_session`>Session beenden</a>

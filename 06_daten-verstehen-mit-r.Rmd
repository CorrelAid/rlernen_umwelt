---
title: "Daten verstehen mit R"
author: CorrelAid e.V.
date: "`r Sys.Date()`"
authors:
  - Nina Hauser
  - Zoé Wolter
  - Jonas Lorenz
  - Sylvi Rzepka
  - Susan Reichelt
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: flatly
    css: www/style.css
    includes:
      after_body: ./www/favicon.html
    language: de
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(learnr)
library(gradethis)
library(ggplot2)

source("R/setup/gradethis-setup.R")
source("R/setup/tutorial-setup.R")
# Read app parameters
params <- yaml::yaml.load_file("www/app_parameters.yml")

# Benötigte Daten laden
source("R/setup/functions.R")
community <- get_community()
data_raw <- get_data_raw()
```

```{r results='asis'}
cat(get_license_note(rmarkdown::metadata$title, rmarkdown::metadata$authors))
```

Wir wollen uns heute mithilfe von R einen Überblick über unsere Daten verschaffen. Wie uns das mit ein paar grundlegenden Schritten gelingt, werden wir anhand einiger Codes "in action" sehen. Wer möchte, kann diesen Code direkt replizieren, ansonsten soll es und erst einmal darum gehen, mit kleinen Handgriffen die Daten greifbar zu machen. Beginnen wir mit dem Video:

![*Video: Daten mit R verstehen (20min)*](https://youtu.be/nEbiJ4EZ3OE)

# **Daten verstehen mit R**

## **Zusammenfassung: Grundlagen der Datenanalyse**

  - **Überblick**: Um explorative Datenanalysen durchführen zu können ist es wichtig, einen umfassenden Eindruck der Daten zu erhalten. Das gelingt uns, indem wir Muster erkennen und Extremwerte identifizieren können.
  - **Vorgehen**: Eine erste Datenanalyse lässt sich in drei grundlegende Schritte aufteilen. 
      1. Überblick verschafffen mithilfe **niedrigschwelliger** Zusammenfassungen der Daten. 
      2. Untersuchung der Variablen mithilfe **einfacher Datenvisualisierungen**
      3. Weiterführende Analysen auf Basis der Berechnung **statistischer Kennzahlen**, (d.h. Lage- und Verteilungsparameter)

So weit so gut. Allerdings gibt es noch eine Sache, die wir bei der Datenanalyse und -interpretation immer beachten müssen: **Den Kontext**! Ganz nach dem Grundsatz **Context is key!** achten wir immer darauf, dass wir unsere Daten immer im Kontext zu den angewandten statistischen und visuellen Auswertungen bewerten. Dazu gehört ebenfalls, dass wir externe Rahmenbedingungen unserer Daten berücksichtigen: Woher kommen unsere Daten? Zu welchem Zweck wurden die Daten erhoben und aus welcher Zeit stammen die Daten? Wie könnten sich diese Aspekte auf unsere Daten auswirken?

## **Einstieg: Unsere Tools in R**

Keine Sorge, wir werden uns zu einem späteren Zeitpunkt noch einmal ganz ausführlich anschauen, wie wir spannende Visualisierungen erstellen, Daten(-formate) bearbeiten oder Reports erstellen können. Hier an dieser Stelle wollen wir uns erst einmal anschauen, wie wir in relativ kurzer Zeit einige Fragen gezielt beantworten können.

Dafür benötigen wir zunächst einmal zwei Packages: **`dplyr`** und **`ggplot2`**. Beide Packages sind Teil des sogenannten `tidyverse`-Packages. Vergangene Woche haben wir bereits einmal davon gehört und auch in den beiden Sessions zur "Datentransformation" wird uns dieses Package wieder begleiten - es kommen also noch viele spannende Dinge auf uns zu!
Für diese Woche werden wir zu allererst einmal wie gewohnt, die beiden Packages installieren und laden:

```{r pakete_ersteanalysen, exercise = TRUE}
# install.packages("dplyr")
# install.packages("ggplot2")

library(dplyr)
library(ggplot2)
```
---

Bevor wir nun direkt in unsere praktische Übung starten, wollen wir uns einmal kurz an unsere Lektion zu den **Grundlagen der Statistik** zurück erinnern. Denn viele Themen, Begriffe und Konzepte, die wir dort bereits kennengelernt haben, werden wir nun mit R umsetzen. 

## **Quiz**

```{r quiz_ersteanalyseninr}
quiz(caption = NULL,
  question("Der Mittelwert ist eine statistische Kennzahl. Er ist robust gegenüber Ausreißern.",
    answer("Wahr."),
    answer("Unwahr.", correct = TRUE),
    correct = "Richtig, der Mittelwert ist eine Kennzahl, die wir umgangssprachlich gerne als Durchschnitt bezeichnen. Zur Berechnung summieren wir alle Werte auf und dividieren die Summe anschließend durch die Anzahl unserer Werte. Extremwerte - positive wie negative - haben dadurch einen großen Einfluss auf unseren Mittelwert. Wenn wir also das "wahre" Mittel unserer Werte bestimmen wollen, berechnen wir idealerweise den Median, dieser ist robust gegenüber Ausreißern!",
    incorrect = "Leider falsch, der Mittelwert ist eine Kennzahl, die wir umgangssprachlich gerne als Durchschnitt bezeichnen. Erinnere Dich noch einmal daran, wie wir den Mittelwert berechnen: Welche Folgen könnten extreme positive und negative Werte auf diese Wert haben? Welche Maßzahl könnten wir stattdessen berechnen, wenn wir das "wahre" Mittel unserer Verteilung herausfinden möchten?",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),

  question("Wir haben die Verteilung einer Zufallsvariable: Der Mittelwert dieser Variable 5, die Spannweite 10 und die Standardabweichung 2. Um wie viel, weichen die Beobachtungen im Mittel von dem Wert 5 ab?",
    answer("2", correct = TRUE),
    answer("5"),
    answer("10"),
    correct = "Richtig, die Standardabweichung ist eine Maßzahl, die die mittlere Streuung bzw. Abweichung unserer Daten um den Mittelwert einer Variable angibt - in diesem Fall also um den Wert 2. Die Einheit der Standardabweichung entspricht dabei der Maßeinheit unserer Variable (Reminder: Die Varianz ist die quadrierte Standardabweichung und wird daher in einer anderen Einheit angegeben).",
    incorrect = "Leider falsch, die Standardabweichung ist eine Maßzahl, die die mittlere Streuung bzw. Abweichung unserer Daten um den Mittelwert einer Variable angibt - in diesem Fall also um den Wert 2. Die Einheit der Standardabweichung entspricht dabei der Maßeinheit unserer Variable (Reminder: Die Varianz ist die quadrierte Standardabweichung und wird daher in einer anderen Einheit angegeben).",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),

  question("Was meint Ihr, welche Aussagen sind wahr?",
    answer("Die Berechnung statistischer Kennzahlen reicht aus, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Die Erstellung von Visualisierungen reicht aus, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Der Kontext ist bei der Analyse und Interpretation von Daten nicht so wichtig."),
    answer("Als Grundlage der Analyse und Interpreationen von Daten dienen uns die Berechnung statistischer Kennzahlen, sowie die Visualisierung und Kontextualisierung.", correct = TRUE),
    correct = "Richtig, es reicht eben nicht aus, Kennzahlen zu berechnen und bunte Grafiken zu erstellen. Statistische Maße und qualitative Visualisierungen sind wichtig und sind Teil einer umfassenden Datenanalyse. Während des gesamten Prozesses von der Datenanalyse bis hin zur Interpretation dürfen wir allerdings eines nicht aus den Augen verlieren: Den Kontext!",
    incorrect = "Leider falsch, die Berechnung statistischer Kennzahlen und die Visualisierung dieser sind ein wichtiger Teil einer umfassenden Datenanalyse. Während des gesamten Prozesses von der Datenanalyse bis hin zur Interpretation dürfen wir aber eines nicht vergessen: Den Kontext. Alle drei Dinge sind die Grundlage unserer Arbeit!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

# **Interaktive Übung**

## **1. Schritt der Datenanalyse: Fragen**

Am Anfang jeder Datenanalyse steht etwas, dass zunächst gar nichts mit Daten zu tun haben muss: **Fragen**. Daten sind schließlich nichts anderes als **formalisierte Informationen**, die uns in erster Linie zum **Erkenntnisgewinn** verhelfen sollen. Strategische Überlegungen helfen uns dabei, bestimmte Fragestellungen zu formulieren, auf die wir uns anschließend fokussieren können.
Bleiben wir also bei unserem [Datensatz](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-01-26){target="_blank"} von [#Break Free From Plastic](https://www.breakfreefromplastic.org/){target="_blank"} und versetzen uns in die Rolle der Organisator:innen - welche Fragen könnten und müssen wir uns vielleicht sogar stellen? Wir haben hier ein paar Beispiele, ergänzt diese Liste gerne um Eure eigenen Ideen!

  1.  Wie viel Plastik wurde insgesamt gesammelt? <br>
  2.  Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>
  3.  Welche Faktoren beeinflussen möglicherweise diese Unterschiede? <br>
  ... 

## **2. Schritt der Datenanalyse: Datenstruktur**

Nachdem wir diese Fragen formuliert haben, geht es darum, diese bestmöglich zu beantworten. Wir müssen also auf die Informationen schauen, die uns vorliegen. Das machen wir, indem wir unsere **Daten analysieren und interpretieren**! 

*Anmerkung:* Da es in dieser Einheit darum geht, in den **Analyse-Spirit** zu kommen und R als Analysetool kennenzulernen, haben wir den Datensatz "Break Free from Plastic" bereits **bereinigt** und in **zwei Datensätze** aufgeteilt: Einen `community`- und einen `audit`-Datensatz.
  - **`community´**: Dieser Datensatz enhtählt vor allem die Variablen, welche für Fragestellungen rund um die Community-Perspektive nützlich sind. Dazu zählen unter anderem die Anzahl an gesammelten Plastikteilen, die Anzahl der Events und freiwillig Engagierten.
  - **`audit`**: Dieser Datensatz umfasst vor allem jene Variablen, die für Fragen zur Audit-Perspektive nützlich sind. Dazu zählen unter anderem der Name der Firma und die Klassifizierung der Kunststoffe.*

### **Die `glimpse()`-Funktion**

Eine Funktion, die wir verwenden können, um uns einen ersten Überblick zu verschaffen, ist die **`glimpse()`-Funktion** aus dem Package **`dplyr`** (einem Package des `tidyverse` - mehr dazu in der kommenden Woche). Mit dieser Funktion erhalten wir Informationen über die Struktur des Datensatzes wie z.B. die Anzahl der Beobachtungen und Variablen, sowie die Namen und Datentypen der Variablen). Wenn wir nun die Funktion **`dplyr::glimpse()`** ausführen und den Output betrachten, können wir verschiedene Dinge herauslesen, aber versucht das einmal selbst:

```{r exercise_community, exercise = TRUE}
dplyr::glimpse(community)
```

```{r quiz_kurzstatistik}
quiz(caption = NULL,
  question(
    "Wie viele Variablen hat der `community`-Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("51"),
    answer("6", correct = TRUE),
    answer("7"),
    correct = "Richtig, unser Datensatz hat insgesamt sechs Variablen. Wir können das auf den ersten Blick daran erkennen, dass unser Datensatz aus eben seschs Spalten (Columns: 6) besteht (Wiederholung: Eine Spalte = ein Merkmal bzw. eine Variable). In den Zeilen werden im Gegensatz dazu die Beobachtungen abgebildet. In diesem Fall sind die Länder unsere Beobachtungseinheiten (also: eine Zeile = ein Land).",
    incorrect = "Leider falsch, schau' nochmal einmal genauer hin und versuche dich daran zu erinnern, welche unterschiedlichen Informationen in den Zeilen und Spalten eines Datensatzes abgebildet werden - wir haben darüber in der Session "Grundlagen der Statistik" gesprochen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Welche Variablen gibt die Anzahl an gesammelten Plastikstücken an?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("n_volunteers"),
    answer("n_pieces", correct = TRUE),
    answer("n_events"),
    correct = "Richtig, die Variable "n_pieces" zeigt an, wie viele Plastikteile in dem jeweiligen Land gesammelt wurden. Wenn Du noch einmal eine Übersicht über die Bedeutung aller Variablennamen haben möchtest, kannst Du gerne jederzeit in die Zusammenfassung des Datensatzes schauen, die wir für Euch in der Session "Einfürhung in R" erstellt haben!",
    incorrect = "Leider falsch, schau' noch einmal genauer in die Übersicht, die dplyr::glimpse() ausgibt. Wenn Du noch einmal eine Übersicht über die Bedeutung aller Variablennamen haben möchtest, kannst Du gerne jederzeit in die Zusammenfassung des Datensatzes schauen, die wir für Euch in der Session "Einfürhung in R" erstellt haben!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
    )
  )
```

Fassen wir also noch einmal die Informationen zusammen, die wir auf den ersten Blick erkennen können:

  - Anzahl der Zeilen (Rows: 51) und Spalten (Columns: 6): Eine Zeile ist eine Beobachtung, also in diesem Fall ein Land und eine Spalte steht für eine Variable
  - Bezeichnungen der Spalten, also die Namen der Variablen ($ continent, $ country, ...)
  - Ausschnitt der Ausprägungen der Variablen (z.B. für n_pieces: 988, 5818, 53, ...)

## **3. Schritt der Datenanalyse: Inhaltlicher Überblick**

Einen ersten Überblick über die Grundstruktur unseres Datensatzes konnten wir uns bereits verschaffen, aber so richtig detailliert ist das Ganze jetzt noch nicht. Mehr Informationen liefert uns eine neue Funktion!

### **Die `summary()`-Funktion**

Die **`summary()`-Funktion** ermöglicht uns in kurzer Zeit eine **Zusammenfassung** über die Datenstruktur und -inhalte. Sie ist Teil des Base R-Packages, also bereits automatisch vorinstalliert. Sie fasst dabei grundlegende statistische Informationen zusammen und kann für Vektoren, Listen, Modelle oder ganze Datensätze verwendet werden. Sie fasst einzelne Spalten zusammen und liefert uns, je nach Datentyp, verschiedene Informationen. Schaut Euch doch den folgenden Output selbst einmal an: Welche Datentypen gibt es und welche Informationen stehen uns somit zur Verfügung? Welche Aussagen lassen sich treffen?

```{r summary_community, exercise = TRUE}
# Zusammenfassung
summary(community)
```

``{r quiz_plausibilisierung}
quiz(
  caption = NULL,
  question(
    "Was ist der niedrigste Wert, den die Variable n_pieces annimmt?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("68.5"),
    answer("1.0", correct = TRUE),
    answer("120646.0"),
    correct = "Richtig, werfen wir einen Blick auf die Variable 'n_pieces' und betrachten den ersten Wert, der als 'Min.' abgekürzt wird: Diese Abkürzung steht für 'Minimum' und gibt somit den kleinsten Wert unserer Variable an - in diesem Fall beträgt dieser 1!",
    incorrect = "Leider falsch, wir betrachten ausschließlich die Variable 'n_pieces'. Die Informationen werden zwar durch Abkürzungen dargestellt, aber welche könnte für den niedrigsten Wert (auch: 'Minimum') stehen?",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Die Spannweite gibt die Differenz zwischen dem größten und dem kleinsten Wert einer Variable an. Wie plausibel findet Ihr die Spannweite der Variable n_pieces?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("Viel zu groß."),
    answer("Plausibel", correct = TRUE),
    answer("Viel zu klein"),
    correct = "Richtig, die Spannweite erscheint durchaus plausibel! Es gibt schließlich keine negativen Werte und auch die Größenordnung der Werte ist vorstellbar.",
    incorrect = "Leider falsch, betrachte noch einmal die Werte und achte darauf ob negative oder unrealistisch hohe Werte vorhanden sind. Wenn das in Ordnung aussieht, ist die Spannweite durchaus plausibel.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
    )
  )
```

Wir sehen, dass die Variablen, also die Spalten in unserem Datensatz, unterschiedliche Formate haben. `continent`, `country` und `countrycode` haben die Class “character”, weil sie aus Text/ Buchstaben bestehen. Statistisch gesehen sind sie **nominal skalierte** Variablen, da sie Kategorien sind, ohne Ranking und ohne numerischen Wert. Alle anderen Variablen `n_...` sind **metrisch skalierte** Variablen. Wir können sie der Größe nach sortieren und mit ihnen ganz natürlich rechnen.

### **Die `sum()`-Funktion**

Durch die `summary()`-Funktion haben wir schon einige Informationen über die numerischen Werte (`n_...`) erhalten. Allerdings fehlen uns noch ein paar spannende Informationen wie die **Gesamtsumme** der gesammelten Plastikteile, durchgeführten Events oder freiwillig Engagierten. Die **`sum()`**-Funktion ist ebenfalls bereits vorinstalliert und liefert uns eben all diese Informationen. Mit dem `$`-Zeichen können wir innerhalb der Funktion auf bestimmte **Variablen verweisen** (Logik: `sum(datensatz$variable)`).

```{r berechnung_summen, exercise = TRUE}
# Gesamtsumme
sum(community$n_pieces)
sum(community$n_events)
sum(community$n_volunteers)
```

### **Die `summarize()`-Funktion** 

Als nächstes wollen wir wir Daten in Tabellen zusammenfassen, um beispielsweise neue Datenobjekte zu erstellen. Dabei greifen wir auf eine weitere Funktion aus dem `dplyr`-Package zurück: Die **`summarize()`**-Funktion (auch: `summarise()`). Mit dieser Funktion können wir auf Basis der bereits vorhandenen Variablen neue **Hilfsvariablen** erstellen, die Berechnungen für uns zusammenfassen. Im folgenden Beispiel erstellen wir eine Übersicht, über die Gesamtzahl, sowie die Spannweite der gesammelten Plastikteile. Hierzu erstellen wir die neuen Hilfsvariablen "Gesamt" und "Spannweite" und weisen diesen dann die gewünschte Funktion oder Funktionskombination zu:

```{r summarise, exercise = TRUE}
dplyr::summarize(community, 
                 Gesamt = sum(community$n_pieces),
                 Spannweite = max(community$n_pieces) - min(community$n_pieces))
```
*Übersetzung: Nutze den Datensatz 'community' und erstelle eine Variable "Gesamt", die alle gesammelten Plastikteile aufsummiert und eine Variable "Spannweite", die die Differenz aus Maximum und Minimum der Variable 'n_pieces' berechnet.* 

### **`summarize()` und `group_by()`**

Leider wurden nun alle Länder zusammengeschmissenwodurch wir natürlich keine Muster erkennen können. Dafür müssen wir die `summarize()`-Funktion um die **`group_by()`**-Funktion ergänzen. Beide werden gerne miteinander kombiniert, da sie in recht kurzer Zeit etwas umfassendere Zusammenfassungen über die Daten erstellen. Die `group-by`-Funktion erlaubt es uns, Daten nach den Werten einer oder mehrerer Spalten zu kategorisieren und somit Gruppenanalysen zu erstellen. 
Wir müssen also lediglich den oberen Code ein wenig ergänzen:

```{r Vorschau_group_by, exercise = TRUE}
# Kombination group_by und summarize
dplyr::summarize(group_by(community, Kontinent = community$continent), # aus den Daten 'community' eine Kategorie nach 'continent'
                 Länder = n(), # Anzahl der Zeilen
                 Gesamt = sum(n_pieces), # Summe der Werte
                 Spannweite = max(n_pieces) - min(n_pieces)) # Differenz max() und min()
```

Mit `group_y()` legen wir fest, wir auf den Datensatz `community` zurückgreifen die Daten anhand der neuen Hilfsvariable 'Kontinent' kategorisieren wollen. Da wir uns fürländerübergreifende Unterschiede interesseieren, erstellen wir eine weitere Hilfsvariable 'Länder'.

## **Vorsicht: Interpretation der Daten!**

Wir können bereits jetzt schon wertvolle Erkenntnisse aus unseren Daten gewinnen, dennoch müssen wir bei der Interpretation unserer Daten stets deren **Aggregationsebene** im Auge behalten. Man verweist in diesem Zuge auch auf die **Granularität der Daten**: Je aggregierter Daten vorliegen, desto schwieriger wird es, Zusammenhänge korrekt zu bestimmen. Es kommt also auch hier wieder auf den **Kontext** drauf an!
In unserem Datensatz können wir beispielsweise lediglich Vergleiche zwischen verschiedenen Ländern anstellen - aber können wir auch Aussagen über individuelle Events treffen? Nein, denn auf dieser Ebene liegen uns schlicht keine Informationen vor! Diese Schwankungen und ihre möglichen Ursachen müssen wir also stets bedenken. Die Anzahl der durchgeführten Events weicht beispielsweise stark von der Anzahl involvierter Helfer:innen ab. Wie genau diese Unterschiede zu werten sind, ob beide Zahlen von anderen Variablen (sog. Störfaktoren, engl. *confounding variables*) beeinflusst werden und wir diese überhaupt auf Basis unserer Daten bewerten können müssen wir noch herausfinden. 
Ein Schlagwort welches häuftig in Verbindung mit der Aggregation von Daten fällt, ist der **"Ökologische Fehlschluss"**. Dieser beschreibt, wie auf der Basis von Aggregatdaten unzulässigerweise auf die Individualebene geschlossen wird. Mehr dazu findet Ihr [hier](https://martin-grellmann.de/oekologischer-fehlschluss).

## **4. Schritt der Datenanalyse: Visualisierung**

Wir haben uns nun bereits in Form von Tabellen einen generellen Überblick verschaffen können. Diese Kennzahlen sind wichtig. Jedoch kann es bei so vielen Zahlen schnell mal etwas unüberischtlich werden. Mit einfachen **Datenvisualisierungen** können wir unsere Daten grafisch darstellen und statistische Muster sichtbar machen. <br>
In diesem explorativen Schritt der Datenanalyse, wollen wir uns also einmal mit dem Erkennen von Datenverhalten und -mustern beschäftigen - der technische Hintergrund der Diagramme (engl. Plots) wird uns dann in einer eigenen Session beschäftigen.

### **Grafik: Das Punktdiagramm**

Bleiben wir an dieser Stelle einmal bei den verschiedenen Kontinenten. Mithilfe des Packages **`ggplot2`** können wir hierfür ein sogenanntes **Punktediagramm (engl. Scatterplott)** erstellen. Mit diesem Package kann man verschiedene Grafiken über eine Art **Schichtsystem** erstellen: <br>
1. Schicht: Verweis auf den verwendeten Datensatz (`data = community`) und die Variablen, die entlang der x- und y-Achsen dargestellt werden sollen (`aes(x = continent, y = n_pieces)`).<br>
2. Schicht: Bestimmung der Art der Visualisierung durch die **`geom_jitter()`-Funktion**. 'jitter' beschreibt eine besondere Art von Punktediagramm, bei dem Datenpunkte sich ausweichen um die Lesbarkeit bei Überlagerung zu ermöglichen.<br>
3. Schicht: Beschriftung der Grafik mithilfe der **`labs()`-Funktion**. In diesem Schritt werden Überschrift(en), Achsen und Bilderunterschrift mit Labels versehen.<br>
4. Schritt: Feinschliff und **Layout** durch andere Schichten, die mit einem "+" hinzugefügt werden. Diese beinhalten in der Regel Funktionen zur Gestaltung (z.B. Annotation, Layout, etc.)

Betrachtet nun die folgende Visualisierung, die sowohl die Verteilung als auch die Häufigkeit von Beobachtungen über die Verteilung darstellt. Nehmt Euch einen Moment und beschreibt den Plot in Euren Worten.

```{r geom_point_n_pieces_bericht, exercise = TRUE}
# Erstellung eines jitter plots zur Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community,      # Definition Datensatz
       aes(x = continent,     # Definition x-Achse
           y = n_pieces)) +   # Definition y-Achse
  geom_jitter(size = 3,       # Größe der Punkte
             alpha = 0.6,     # Transparenz der Punkte
             width = 0.2) +   # Breite der Punkt-jitter pro Kategorie
  labs(title = "Auch die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent und Land.",
    y = "Anzahl gefundener Plastikstücke pro Land",
    x = "Kontinent",
    caption = "Datenquelle: TidyTuesday und BFFP") +   # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() +                                    # Festlegung des Layout-Designs  
  theme(legend.position="none")                        # Ausblenden der Legende
```

```{r quiz_scatterplot}
quiz(caption = NULL,
question(
    "In welchen Kontinenten beobachten wir extreme Werte, sogenannte 'Ausreißer'?",
    answer("Afrika und Amerika"),
    answer("Afrika und Europa"),
    answer("Afrika und Asien", correct = TRUE),
    answer("Afrika und Ozeanien"),
    correct = "Richtig, wir können ganz deutlich erkennen, dass die meisten Punkte für alle Kontinente relativ nah aneinander liegen. Lediglich für Afrika und Asien stellen wir einzelne (positive) Ausreißer fest.",
    incorrect = "Leider falsch, beztrachte noch einmal die Grafik: Wo liegen die Punkte alle relativ nah aneinander und wo gibt es einzelne Datenpunkte, die ziemlich weit von den anderen entfernt sind?",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```

### **Grafik: Der Boxplot**

Den **Boxplot** zur Darstellung der **fünf Punkte einer Verteilung** (Minimum, 25%-Quartil, Median, 75%-Quartil, Maximum) kennen wir bereits! Aber Vorsicht, denn "einfache" Boxplots zeigen ausschließlich die statistischen Werte, nicht aber die einzelnen Datenpunkte. Diese sind aber wichtig um zu erkennen, wie unsere Verteilung tatsächlich aussieht (bspw. ob sich die "Box" aus 4 oder 400 Werten zusammensetzt) - Stichwort: **Kontext**! In R lassen sich Boxplots und Scatterplots zum Glück gut kombinieren.

*(Hinweis: wie Ihr auch so einen Boxplot erstellen könnt, lernt Ihr in der Lektion zu Datenvisualisierung! Hier geht's ja erstmal darum, zu sehen, wie uns R helfen kann Daten zu lesen, zu kontextualisieren und zu interpretieren.)*

```{r boxplot_plastik}
# Erstellung eines Boxplots mit Scatterplot zur Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, 
       aes(x = continent, # x-Achse
           y = n_pieces,  # y-Achse
           fill = continent)) + # Farb-füll-Variable
  geom_boxplot(alpha = 0.6) + # Hinzufügen des Boxplots
  geom_point(size = 3, # Größe der Punkte
              alpha = 0.4, # Transparenz der Punkte
              width = 0.1) +  # Breite der Punkt-jitter pro Kategorie
  coord_cartesian(ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + # Festlegung der Achsenlänge der y-Achse abhängig von Median und Standardabweichung
  labs(
    title = "Die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl gefundener Plastikstücke",
    x = "Kontinent",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none") + # Ausblenden der Legende
  scale_fill_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67")) # Anwendung der BFFP-Farben
```

## **5. Schritt: Die Interpretation der Daten**

Werfen wir also einen Blick auf unsere Grafik: Welche Gemeinsamkeiten und Unterschiede können wir feststellen? Wo gibt es Besonderheiten? Welche Dinge sind (noch) unklar? Sammelt Eure Ideen und Fragen, damit wir diese am Freitag in der Live-Session besprechen können!
*Tipp: Wenn Ihr Euch nicht mehr sicher seid, schaut doch nochmal in die Lektion zu den Grundlagen der Statistik.*

---

<details>
<summary><h1><b>Exkurs: Beziehungen zwischen Variablen</b></h1></summary>
  <br> 

Bislang haben wir uns vor allem die Verteilung einer einzelnen Variable (z.B. `n_pieces`) gewidmet. Man spricht in einem solchen Fall auch von einer **univariaten** Verteilung, d.h. dass lediglich eine Variable für sich betrachtet wird. Häufig interessieren wir uns nun aallerdings für die Beziehung zwischen verschiedenen Variablen: Welche Faktoren beeinflussen beispielsweise die Unterschiede in der Anzahl an Plastikstücken, die gesammelt wurden? Die Anzahl durchgeführter Events? Oder doch eher die Anzahl an Freiwilligen im Land?
Wenn wir in diesem Zusammenhang zwei Variablen (z.B. Anzahl Plastikteile und Anzahl Events) betrachten, sprechen wir dabei von einer **bivariaten** Beziehung. Wir könnten allerdings auch mehr als zwei Variablen betrachten (z.B. Anzahl Plastikteile, Anzahl Events und Anzahl der Freiwilligen im Land) - dann würden wir von einer **mutlivariaten** Beziehung sprechen. 

Wir fragen uns also zum Beispiel, ob zwischen zwei Variablen ein Zusammenhang besteht, d.h. ob diese Variablen miteinander **korrelieren**. Mithilfe eines Scatterplots können wir diese Frage beantworten. Wir testen nämlich, ob die Anzahl an Events und/oder die Anzahl an Freiwilligen einen Einfluss auf die Anzahl der gefundenen Plastikstücke haben. Die rote Hilfslinie zeigt uns das Ausmaß der Korrelation an. Vergleichen wir mal...

<div style='float: left; width: 50%;'>
```{r scatter_plot_n_events, fig.width=4,fig.height=3}
# Optional: Erstellung eines Punktediagramms mit der Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, aes( x = n_events, 
                              y = n_pieces)) + 
  geom_point(size = 3,
             alpha = 0.5, 
             color = "darkgrey") + 
  geom_smooth(method = "lm", 
              colour = "darkred", 
              alpha = 0.5, 
              size = 1.5,
              se = F) + # Trendlinie hinzufügen, ohne Standardabweichung (se)
  coord_cartesian(xlim = c(0, median(community$n_events) + 2 * IQR(community$n_events)), 
                  ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + 
  # Festlegung der Achsenlänge der y-Achse abhängig von Median und Interquartilabstand
  labs(
    title = "Anzahl gesammelter Plastiksstücke bei \n'Break Free From Plastic' ..." ,
    subtitle = "... in Abhängigkeit von der Eventanzahl.",
    x = "Events",
    y = "Anzahl gefundener Plastikstücke",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() # Festlegung des Layout-Designs
```

</div>

<div style='float: right; width: 50%;'>
```{r scatter_plot_n_volunteers, fig.width=4,fig.height=3}
# Optional: Erstellung eines Punktediagramms mit der Anzahl gesammelter Plastikstücke pro Kontinent
ggplot(data = community, aes( x = n_volunteers, 
                              y = n_pieces)) + 
  geom_point(size = 3,
             alpha = 0.5, 
             color = "darkgrey") + 
  geom_smooth(method = "lm", 
              colour = "darkred", 
              alpha = 0.5, 
              size = 1.5,
              se = F) + # Trendlinie hinzufügen, ohne Standardabweichung (se)
  coord_cartesian(xlim = c(0, median(community$n_volunteers) + 2 * IQR(community$n_volunteers)), 
                  ylim = c(0, median(community$n_pieces) + 2 * IQR(community$n_pieces))) + 
  # Festlegung der Achsenlänge der y-Achse abhängig von Median und Interquartilabstand
  labs(
    title = "Anzahl gesammelter Plastiksstücke bei \n'Break Free From Plastic' ..." ,
    subtitle = "... in Abhängigkeit von der Anzahl der Freiwilligen",
    x = "Freiwillige Helfer*innen",
    y = "Anzahl gefundener Plastikstücke",
    caption = "Einige Ausreißer wurden zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() # Festlegung des Layout-Designs
```
</div>

Uns fällt direkt auf, dass die rote Gerade im rechten Diagramm deutlich steiler ist, als die Gerade im linken Diagramm. Anhand der Steigung erkennen wir, dass der Zusammenhang zwischen der Anuahl an Helfer:innen und der Anzahl an Plastikteilen deutlich größer ist als zwischen der Anzahl an Events und den gefundenen Plastikteilen. Diese Trendlinie zeigt nämlich an, dass mit der Zunahme von einem Faktor, auch der andere ansteigt. Aber Achtung, denn unser Diagramm sagt Nichts über einen **Ursache-Wirkungs-Zusammenhang aus**! Wir können also nicht mit Sicherheit sagen, dass mehr Freiwillige die **Ursache** für mehr gesammelte Plastikstücken sind - wir beobachten beide Entwicklungen lediglich gleichzeitig. Wir müssen außerdem beachten, dass wir diese Aussage nur auf Länderebene treffen können. Wir können daher nicht darauf schließen, dass Events mit mehr Freiwilligen mehr Plastikstücke sammeln. Wir erinnern uns an das Stichwort **"Ökologischer Fehlschluss"** von oben.

Für diese Unterschiede kann es vielfältige Gründe geben, die wir so nicht unbedingt beobachten können - zum Beispiel die Bevölkerungsanzahl oder auch die Zeit, welche einzelne Freiwillige in die Aktion investiert haben. Welche anderen "Drittvariablen" fallen Euch noch ein?

</details>

---

# **Und jetzt Ihr!**
Diese Woche möchten wir die Präsenzzeit nutzen, um die folgenden Übungen zu besprechen. Ergänzt unseren Input gerne zudem mit Euren **Ideen, Fragen, Anregungen oder Kommentaren**. Es ist nicht schlimm, falls diese Woche noch gar nichts (komplexes) klappt, da wir das Gelernte in den nächsten Wochen wiederholen und vertiefen werden.

1.  Beantwortet anhand der präsentierten Datenanalyse folgende Fragen: <br>
  
  - Wie viel Plastik wurde insgesamt gesammelt? <br>
  - Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>
  - Welche Faktoren beeinflussen möglicherweise diese Unterschiede? <br>

2.  Überlegt: Mit welchen Daten und Datenanalysen könnte die Frage "Wie erfolgreich war der Audit?" noch beantwortet werden? Wie könnte eine Visualisierung oder eine zusammenfassende Statistik dabei helfen? Skizziert Eure Fragen gerne schriftlich.

3.  Versucht, die zugehörige **R Datei: 06_daten-verstehen-mit-r-uebung.R** im [Übungsordner](https://download-directory.github.io/?url=https://github.com/CorrelAid/rlernen_umwelt/tree/main/uebungen){target="_blank"} unter 06_daten-verstehen-mir-r-uebung.R zum Laufen zu bringen und nachzuvollziehen.

4.  In der ersten Einheit haben wir uns bei der Visualisierung vor allem der `n_pieces`-Variable gewidmet. Nun blicken wir auf die **`n_volunteers`**: Wie sehr unterscheiden sich die Freiwilligenzahlen nach Kontinenten? Erstellt in dem heruntergeladenen RMarkdown ein **Punktediagramm** (Scatterplot) mit dem Datensatz `community` für diesen Blickwinkel auf den Erfolg der "Break Free from Plastic" Aktion. Die Graphik soll `n_volunteers` auf der y-Achse und `continents`auf der x-Achse abbilden. Verseht beide Achsen mit den entsprechenden Labels **"Anzahl der Freiwilligen"** und **"Kontinente"**.
*Hinweis: Versucht dazu in der Übungsdatei in der finalen Version der Graphik die entsprechenden Variablen auszutauschen.*

5.  Versuchen wir uns an der Interpretation der Graphik. Was können wir aus unseren Darstellungen ablesen?

# **Zusätzliche Ressourcen**

-   [R for Data Science 2e](https://r4ds.hadley.nz/){target="_blank"}
-   [Stocker T. C. und Steinke I. (2017): Statistik – Grundlagen und Methodik](https://www.beck-shop.de/stocker-steinke-de-gruyter-studium-statistik/product/32926361){target="_blank"}
-   [Statistics Fundamentals in R](https://app.dataquest.io/course/statistics-fundamentals-r){target="_blank"} auf DataQuest
-   [Statistik-Lernvideos](https://www.youtube.com/watch?v=RRIsBFW8ovc){target="_blank"} von "Kurzes Tutorium Statistik"
-   [Datenbank für umweltbezogene Datenprojekte](https://www.umweltbundesamt.de/publikationen){target="_blank"} vom Umweltbundesamt

<a class="btn btn-primary btn-back-to-main" href=`r params$links$end_session`>Session beenden</a>
